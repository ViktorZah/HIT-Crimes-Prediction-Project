{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataclasses\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "import graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "import geocoder\n",
    "import dataclasses\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import matplotlib as mat\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Row:\n",
    "    _id: str\n",
    "    case_number: str\n",
    "    date: str\n",
    "    block: str\n",
    "    IUSR: str\n",
    "    primary_type: str\n",
    "    description: str\n",
    "    location_description: str\n",
    "    arrest: str\n",
    "    domestic: str\n",
    "    beat: str\n",
    "    district: str\n",
    "    ward: str\n",
    "    community_area: str\n",
    "    fbi_code: str\n",
    "    x_coordinate: str\n",
    "    y_coordinate: str\n",
    "    year: str\n",
    "    updated_on: str\n",
    "    latitude: str\n",
    "    longitude: str\n",
    "\n",
    "    def location(self) -> (str, str):\n",
    "        return self.longitude, self.latitude\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "       '_id': self._id,\n",
    "       'case_number': self.case_number,\n",
    "       'date': self.date,\n",
    "       'block': self.block,\n",
    "       'IUSR': self.IUSR,\n",
    "       'primary_type': self.primary_type,\n",
    "       'description': self.description,\n",
    "       'location_description': self.location_description,\n",
    "       'arrest': self.arrest,\n",
    "       'domestic': self.domestic,\n",
    "       'beat': self.beat,\n",
    "       'district': self.district,\n",
    "       'ward': self.ward,\n",
    "       'community_area': self.community_area,\n",
    "       'fbi_code': self.fbi_code,\n",
    "       'x_coordinate': self.x_coordinate,\n",
    "       'y_coordinate': self.y_coordinate,\n",
    "       'year': self.year,\n",
    "       'updated_on': self.updated_on,\n",
    "       'latitude': self.latitude,\n",
    "       'longitude': self.longitude,\n",
    "       'location': self.location\n",
    "        }\n",
    "\n",
    "website = 'https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present-Dashboard/5cd6-ry5g'\n",
    "path = '\\Desktop'\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(website)\n",
    "time.sleep(10)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(1)\n",
    "cell_counter = 0\n",
    "attrs = []\n",
    "rows = []\n",
    "\n",
    "the_table = driver.find_element(By.CLASS_NAME, 'table-body')\n",
    "driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", the_table)\n",
    "scroll_increment = 20000\n",
    "for _ in range(5):\n",
    "    scroll_script = f\"arguments[0].scrollTop += {scroll_increment};\"\n",
    "    driver.execute_script(scroll_script, the_table)\n",
    "    time.sleep(5)  # Wait for the new data to load\n",
    "\n",
    "# Get the total number of elements      #div table-body\n",
    "total_elements = len(driver.find_elements(By.CLASS_NAME, 'cell-content'))\n",
    "time.sleep(60)\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=total_elements, desc='Progress', unit='col')\n",
    "\n",
    "for content in driver.find_elements(By.CLASS_NAME, 'cell-content'):\n",
    "    try:\n",
    "        content = content.text\n",
    "        attrs.append(content)\n",
    "        cell_counter += 1\n",
    "\n",
    "        if cell_counter > 21:\n",
    "            attrs.pop()\n",
    "            rows.append(Row(*attrs))\n",
    "            attrs = []\n",
    "            cell_counter = 0\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "df = pd.DataFrame([obj.to_dict() for obj in rows])\n",
    "df.to_csv('testingData.csv')\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#########################good data cleaning ##################################################################\n",
    "\n",
    "the_path='D:/programs/CHROM EXECUTEBLES/files for project/testingData.csv'\n",
    "try:\n",
    "    the_file=pd.read_csv(the_path)\n",
    "    print(\"File saved at:\", os.path.abspath('testingData2.csv'))\n",
    "    print(\"File saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while saving the file:\", e)\n",
    "\n",
    "\n",
    "#the_file.head()\n",
    "upgraded_one=the_file.iloc[:,1:]\n",
    "\n",
    "colm_names=upgraded_one.columns.tolist()\n",
    "colm_numbers=len(colm_names)\n",
    "print(colm_numbers)\n",
    "print(colm_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "un_wanted_colms=['IUCR','Block','Beat','District','Ward','Community Area','Year','Updated On','Latitude','Longitude','Police Districts','Boundaries - ZIP Codes','Wards','Census Tracts','Police Beats','Zip Codes','Historical Wards 2003-2015','Community Areas','X Coordinate','Y Coordinate']\n",
    "upgraded_one=upgraded_one.drop(un_wanted_colms,axis=1)\n",
    "\n",
    "for colm in upgraded_one.columns :\n",
    "    dups = upgraded_one[colm].duplicated()\n",
    "    duplicate_count = dups.sum()\n",
    "    if duplicate_count > 0 :\n",
    "        print(f\"Found {duplicate_count} duplicates in column '{colm}'.\")\n",
    "\n",
    "dups=upgraded_one['Case Number'].duplicated()\n",
    "if dups.any():\n",
    "    pgraded_one = upgraded_one.drop_duplicates(subset=[colm[0]])\n",
    "else:\n",
    "    print(\"no dups in Case Number\")\n",
    "\n",
    "\n",
    "for colmm in upgraded_one.columns :\n",
    "    number_of_missing_vals = sum(upgraded_one[colmm].isnull() | (upgraded_one[colmm] == ''))\n",
    "    if number_of_missing_vals > 0 :\n",
    "        print(f\"Column '{colmm}' has {number_of_missing_vals} missing value(s).\")\n",
    "\n",
    "value1=\"UNknown\"\n",
    "\n",
    "replacement_values = {'Location Description' : value1, 'Location' : value1,}\n",
    "upgraded_one = upgraded_one.fillna(replacement_values)\n",
    "\n",
    "\n",
    "column_name = 'Primary Type'\n",
    "unique_values = upgraded_one[column_name].value_counts()\n",
    "\n",
    "# Calculate the percentage of each unique value\n",
    "total_count = unique_values.sum()\n",
    "percentages = unique_values / total_count * 100\n",
    "\n",
    "# Filter out values below the threshold\n",
    "threshold = 1\n",
    "filtered_values = unique_values[percentages >= threshold]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(filtered_values.values, labels=filtered_values.index, autopct='%1.1f%%')\n",
    "plt.title(f'Unique Value Distribution for Column: {column_name} (Threshold: {threshold}%)')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "the_data = upgraded_one['Date'].str.split(' ')\n",
    "upgraded_one['Time'] = the_data.str[1]\n",
    "upgraded_one['Seconds'] = upgraded_one['Time'].str.split(':').str[-1]\n",
    "upgraded_one['Hour'] = upgraded_one['Time'].str.split(':').str[:1].str.join(':')\n",
    "\n",
    "grouped_data = upgraded_one.groupby(['Primary Type', 'Hour']).size().unstack()\n",
    "\n",
    "# Get the top six types\n",
    "top_six_types = upgraded_one['Primary Type'].value_counts().head(6).index\n",
    "\n",
    "# Plot the line chart for each primary type\n",
    "for primary_type in top_six_types:\n",
    "    data = grouped_data.loc[primary_type]\n",
    "    plt.plot(data.index, data.values, marker='o', label=primary_type)\n",
    "\n",
    "# Set the x-axis labels to the actual hour values\n",
    "hours = [datetime.strptime(str(hour), \"%H\").strftime(\"%I %p\") for hour in range(24)]\n",
    "plt.xticks(range(24), hours)\n",
    "\n",
    "# Set the x-axis limits to include all hours\n",
    "plt.xlim(0, 23)\n",
    "\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Correlation between Primary Type and Hour')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "path_to_save='D:/programs/CHROM EXECUTEBLES/files for project/upgraded_file.csv'\n",
    "upgraded_one.to_csv(path_to_save, index=False)\n",
    "\n",
    "print(upgraded_one.columns)\n",
    "print(len(upgraded_one.columns))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########some more cleaning #####\n",
    "\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_saparated.csv'\n",
    "\n",
    "try:\n",
    "     the_file = pd.read_csv(path_to_file)\n",
    "     print(\"File opened successfully.\")\n",
    "except Exception as e:\n",
    "     print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "unic_primary = the_file['Primary Type'].unique()\n",
    "unic_location = the_file['Location Description'].unique()\n",
    "unic_description = the_file['Description'].unique()\n",
    "\n",
    "print(\"XXXXXXXXXXXPrimary Types:XXXXXXXXXXXXXXXXXXXXXX\")\n",
    "print(unic_primary)\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXXLocation Descriptions:XXXXXXXXXXXXXXXXXXXXXX\")\n",
    "print(unic_location)\n",
    "\n",
    "print(\"XXXXXXXXXXXXXXXXXDescriptions:XXXXXXXXXXXXXX\")\n",
    "print(unic_description)\n",
    "\n",
    "\n",
    "mapping_for_primary={\n",
    "     #gun related\n",
    "     'BATTERY' : 'wepon related',\n",
    "    'THEFT' : 'wepon related',\n",
    "    'ASSAULT' : 'wepon related',\n",
    "     'BURGLARY' : 'wepon related',\n",
    "    'ROBBERY' : 'wepon related',\n",
    "    'WEAPONS VIOLATION' : 'wepon related',\n",
    "     'ARSON' : 'wepon related',\n",
    "    'RITUALISM' :'wepon related',\n",
    "\n",
    "     #drug related\n",
    "     'NARCOTICS' : 'NARCO',\n",
    "    'OTHER NARCOTIC VIOLATION' : 'NARCO',\n",
    "     #sexualy related\n",
    "     'SEX OFFENSE' : 'SEXUAL CRIM ',\n",
    "    'PROSTITUTION' : 'SEXUAL CRIM',\n",
    "     'CRIM SEXUAL ASSAULT' : 'SEXUAL CRIM',\n",
    "    'CRIMINAL SEXUAL ASSAULT' : 'SEXUAL CRIM',\n",
    "    'HUMAN TRAFFICKING' : 'SEXUAL CRIM',\n",
    "     'DOMESTIC VIOLENCE':'SEXUAL CRIM',\n",
    "     # light crimes\n",
    "     'DECEPTIVE PRACTICE' : 'LIGHT CRIM',\n",
    "    'OTHER OFFENSE' : 'LIGHT CRIM',\n",
    "    'PUBLIC PEACE VIOLATION' : 'LIGHT CRIM',\n",
    "     'GAMBLING' : 'LIGHT CRIM',\n",
    "    'LIQUOR LAW VIOLATION' : 'LIGHT CRIM',\n",
    "    'NON - CRIMINAL' : 'LIGHT CRIM',\n",
    "     'STALKING' : 'LIGHT CRIM',\n",
    "    'NON-CRIMINAL' : 'LIGHT CRIM',\n",
    "    'NON-CRIMINAL (SUBJECT SPECIFIED)' : 'LIGHT CRIM',\n",
    "     'NON-CRIMINAL (SUBJECT SPECIFIED)' : 'LIGHT CRIM',\n",
    "\n",
    "     # vilence/seriuce crime\n",
    "     'CRIMINAL DAMAGE' : 'vilence/seriuce crime',\n",
    "    'CRIMINAL TRESPASS' : 'vilence/seriuce crime',\n",
    "    'INTERFERENCE WITH PUBLIC OFFICER' : 'vilence/seriuce crime',\n",
    "     'OFFENSE INVOLVING CHILDREN' : 'vilence/seriuce crime',\n",
    "    'KIDNAPPING' : 'vilence/seriuce crime',\n",
    "    'INTIMIDATION' : 'vilence/seriuce crime',\n",
    "     'HOMICIDE' : 'vilence/seriuce crime',\n",
    "    'MOTOR VEHICLE THEFT' :'vilence/seriuce crime',\n",
    "}\n",
    "\n",
    "the_file['Primary Type']=the_file['Primary Type'].map(mapping_for_primary)\n",
    "\n",
    "\n",
    "\n",
    "def replace_keywords(description):\n",
    "    keyword_map = {\n",
    "        'RESIDENCE': 'PRIVET PROP',\n",
    "        'BUS': 'PUBLIC',\n",
    "        'SIDEWALK': 'PUBLIC',\n",
    "        'APARTMENT' : 'PRIVET PROP',\n",
    "        'STORE': 'PUBLIC',\n",
    "        'STREET': 'PUBLIC',\n",
    "        'GARAGE' : 'PUBLIC',\n",
    "        'RESTAURANT' : 'PUBLIC',\n",
    "        'ALLEY' : 'PUBLIC',\n",
    "        'PARK' : 'PUBLIC',\n",
    "        'CLUB' : 'PUBLIC',\n",
    "        'OFFICE' : 'WORK',\n",
    "        'HOTEL' : 'VACATION',\n",
    "        'BAR' : 'PUBLIC',\n",
    "        'TAVERN' : 'PUBLIC',\n",
    "        'CHURCH' : 'PUBLIC',\n",
    "        'ARENA' : 'PUBLIC',\n",
    "        'HOSPITAL' : 'PUBLIC',\n",
    "        'STATION' : 'PUBLIC',\n",
    "        'TRAIN' : 'PUBLIC',\n",
    "        'SHOP' : 'PUBLIC',\n",
    "        'BANK' : 'PUBLIC',\n",
    "        'SCHOOL' : 'PUBLIC',\n",
    "        'PUBLIC' : 'PUBLIC',\n",
    "        'BUILDING' : 'PRIVET PROP',\n",
    "        'AIRPORT' : 'PUBLIC',\n",
    "        'POLICE' : 'GOV',\n",
    "        'GARAGE' : 'PUBLIC',\n",
    "        'MEDICAL':'GOV',\n",
    "        'FARM':'WORK',\n",
    "        'COLLEGE':'PUBLIC',\n",
    "        'SCHOOL':'PUBLIC',\n",
    "        'WATER':'PUBLIC',\n",
    "        'MOTEL':'VACATION',\n",
    "        'CLUB':'PUBLIC',\n",
    "        'BASEMENT':'PRIVET PROP',\n",
    "        'FACTORY':'WORK',\n",
    "        'TRAILER':'PRIVET PROP',\n",
    "        'TAXI CAB':'PUBLIC',\n",
    "        'HOUSE':'PRIVET PROP',\n",
    "        'DOCK':'WORK',\n",
    "        'HALL':'PUBLIC',\n",
    "        'RIVER':'VACATION',\n",
    "        'SEWER':'PUBLIC',\n",
    "        'GOVERNMENT':'GOV',\n",
    "        'DUMPSTER':'PUBLIC',\n",
    "        'HOME':'PRIVET PROP',\n",
    "        'LAKE':'VACATION',\n",
    "        'POLICE':'GOV',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Add more keyword mappings as needed\n",
    "    }\n",
    "    for keyword, replacement in keyword_map.items() :\n",
    "        pattern = re.compile(r'\\b%s\\b' % re.escape(keyword), re.IGNORECASE)\n",
    "        if pattern.search(description) :\n",
    "            description = replacement\n",
    "\n",
    "    return description\n",
    "\n",
    "the_file['Location Description']=the_file['Location Description'].apply(replace_keywords)\n",
    "\n",
    "\n",
    "\n",
    "the_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_saparated_cleaned_latest_version.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### some more cleaning data and prepering for machin learning ############################################\n",
    "\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_saparated_cleaned_latest_version.csv'\n",
    "\n",
    "try:\n",
    "     the_file = pd.read_csv(path_to_file)\n",
    "     print(\"File opened successfully.\")\n",
    "except Exception as e:\n",
    "     print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "\n",
    "how_much_unic_we_got=the_file['Location Description'].unique()\n",
    "print(how_much_unic_we_got)\n",
    "\n",
    "def replace_keywords2(description):\n",
    "    keyword_map = {\n",
    "        'VEHICLE':'PUBLIC',\n",
    "        'TAXICAB':'PUBLIC',\n",
    "        'RESIDENTIAL YARD':'PRIVET PROP',\n",
    "        'HALLWAY/STAIRWELL/ELEVATOR':'PRIVET PROP',\n",
    "        'PARKING LOT':'PUBLIC',\n",
    "        'LIBRARY':'PUBLIC',\n",
    "        'RESIDENTIAL':'PRIVET PROP',\n",
    "        'CONSTRUCTION':'WORK',\n",
    "        'WAREHOUSE':'WORK'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Add more keyword mappings as needed\n",
    "    }\n",
    "    for keyword, replacement in keyword_map.items() :\n",
    "        pattern = re.compile(r'\\b%s\\b' % re.escape(keyword), re.IGNORECASE)\n",
    "        if pattern.search(description) :\n",
    "            description = replacement\n",
    "\n",
    "    return description\n",
    "\n",
    "the_file['Location Description']=the_file['Location Description'].apply(replace_keywords2)\n",
    "how_much_unic_we_got=the_file['Location Description'].unique()\n",
    "print('efter_funcccc')\n",
    "print(how_much_unic_we_got)\n",
    "\n",
    "the_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/test_items_efter.csv',index=False)\n",
    "\n",
    "\n",
    "how_much_unic_we_got=the_file['Description'].unique()\n",
    "print('description from  here xxxxxxxxxx')\n",
    "print(how_much_unic_we_got)\n",
    "\n",
    "def replace_keywords3(description):\n",
    "    keyword_map = {\n",
    "        'DOMESTIC':'HIGH',\n",
    "        'POCKET-PICKING':'LIGHT',\n",
    "        'HEROIN':'HIGH',\n",
    "        'ENTRY':'LIGHT',\n",
    "        'THEFT':'LIGHT',\n",
    "        'STRONGARM':'LIGHT',\n",
    "        'THREAT':'LIGHT',\n",
    "        'WEAPON':'HIGH',\n",
    "        'VEHICLE':'LIGHT',\n",
    "        'PROPERTY':'LIGHT',\n",
    "        'ARMED':'MEDIUM',\n",
    "        'HARASSMENT':'LIGHT',\n",
    "        'OFFENSE':'LIGHT',\n",
    "        'VIOLATION':'LIGHT',\n",
    "        'CANNABIS':'MEDIUM',\n",
    "        'GUN':'MEDIUM',\n",
    "        'HEROIN':'HIGH',\n",
    "        'FIREARM':'MEDIUM',\n",
    "        'CRACK':'MEDIUM',\n",
    "        'CUTTING':'HIGH',\n",
    "        'ATTEMPT':'LIGHT',\n",
    "        'NARCOTICS':'MEDIUM',\n",
    "        'RESIST':'LIGHT',\n",
    "        'SEX':'HIGH',\n",
    "        'ABUSE':'HIGH',\n",
    "        'VIOLATION':'LIGHT',\n",
    "        'COCAINE':'HIGH',\n",
    "        'FRAUD':'LIGHT',\n",
    "        'CRIMINAL':'MEDIUM',\n",
    "        'RESIDENCE':'LIGHT',\n",
    "        'SYNTHETIC DRUGS':'MEDIUM',\n",
    "        'SOLICIT':'LIGHT',\n",
    "        'ENDANGERING':'LIGHT',\n",
    "        'SEXUAL':'HIGH',\n",
    "        'ABANDONMENT':'MEDIUM',\n",
    "        'PREDATORY':'HIGH',\n",
    "        'SIMPLE':'LIGHT',\n",
    "        'HANDGUN':'MEDIUM',\n",
    "        # Add more keyword mappings as needed\n",
    "    }\n",
    "    for keyword, replacement in keyword_map.items() :\n",
    "        pattern = re.compile(r'\\b%s\\b' % re.escape(keyword), re.IGNORECASE)\n",
    "        if pattern.search(description) :\n",
    "            description = replacement\n",
    "    return description\n",
    "\n",
    "try:\n",
    "   the_file['Description'] = the_file['Description'].apply(replace_keywords3)\n",
    "   unique_descriptions = the_file['Description'].unique()\n",
    "   print('Keywords replaced successfully.')\n",
    "except execption as e:\n",
    "    print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "print('new from hereeeeee description from  here xxxxxxxxxx')\n",
    "how_much_unic_we_got=the_file['Description'].unique()\n",
    "print(how_much_unic_we_got)\n",
    "\n",
    "conditions_for_description = ~the_file['Description'].isin(['LIGHT', 'MEDIUM', 'HIGH'])\n",
    "count = the_file[conditions_for_description]['Description'].value_counts()\n",
    "print('desctiption count')\n",
    "print(count)\n",
    "\n",
    "\n",
    "conditions_for_Loc_description = ~the_file['Location Description'].isin(['PRIVET PROP', 'PUBLIC', 'GOV','WORK','VACATION'])\n",
    "count = the_file[conditions_for_Loc_description]['Location Description'].value_counts()\n",
    "print('location count')\n",
    "print(count)\n",
    "\n",
    "conditions_for_Primary = ~the_file['Primary Type'].isin(['wepon related', 'NARCO', 'SEXUAL CRIM','LIGHT CRIM','vilence/seriuce crime'])\n",
    "count = the_file[conditions_for_Primary]['Primary Type'].value_counts()\n",
    "print('primary')\n",
    "print(count)\n",
    "\n",
    "\n",
    "\n",
    "row_count=the_file.shape[0]\n",
    "print(\"number of rows befor drop\",row_count)\n",
    "print(the_file['Primary Type'])\n",
    "print(the_file['Location Description'])\n",
    "print(the_file['Description'])\n",
    "\n",
    "\n",
    "the_file.drop(the_file.loc[conditions_for_Primary].index,inplace=True)\n",
    "the_file.drop(the_file.loc[conditions_for_Loc_description].index,inplace=True)\n",
    "the_file.drop(the_file.loc[conditions_for_description].index,inplace=True)\n",
    "print('XXXXXXXXXXXefter dropXXXXXXX')\n",
    "print(the_file['Primary Type'])\n",
    "print(the_file['Location Description'])\n",
    "print(the_file['Description'])\n",
    "the_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/test_items_try_EDA.csv',index=False)\n",
    "row_count=the_file.shape[0]\n",
    "print(\"number of rows befor drop\",row_count)\n",
    "\n",
    "\n",
    "the_file['Primary Type'] = the_file['Primary Type'].astype('category')\n",
    "the_file['Primary Type'] = the_file['Primary Type'].cat.codes + 1\n",
    "\n",
    "the_file['Location Description'] = the_file['Location Description'].astype('category')\n",
    "the_file['Location Description'] = the_file['Location Description'].cat.codes + 1\n",
    "\n",
    "the_file['Description'] = the_file['Description'].astype('category')\n",
    "the_file['Description'] = the_file['Description'].cat.codes + 1\n",
    "\n",
    "\n",
    "print('XXXXXXXXXXXefter numeric represss XXXXXXX')\n",
    "print(the_file['Primary Type'])\n",
    "print(the_file['Location Description'])\n",
    "print(the_file['Description'])\n",
    "\n",
    "the_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_try_this_for_learning_machin.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### translating long and lat into location name #######################\n",
    "\n",
    "def get_address(address):\n",
    "    neighborhoods = [\n",
    "        \"Albany Park\", \"Archer Heights\", \"Armour Square\", \"Ashburn\", \"Auburn Gresham\", \"Austin\", \"Avalon Park\",\n",
    "        \"Avondale\", \"Belmont Cragin\", \"Beverly\", \"Bridgeport\", \"Brighton Park\", \"Burnside\", \"Calumet Heights\",\n",
    "        \"Chatham\", \"Chicago Lawn\", \"Clearing\", \"Douglas\", \"Dunning\", \"East Garfield Park\", \"East Side\", \"Edgewater\",\n",
    "        \"Edison Park\", \"Englewood\", \"Forest Glen\", \"Fuller Park\", \"Gage Park\", \"Garfield Ridge\", \"Grand Boulevard\",\n",
    "        \"Greater Grand Crossing\", \"Hegewisch\", \"Hermosa\", \"Humboldt Park\", \"Hyde Park\", \"Irving Park\", \"Jefferson Park\",\n",
    "        \"Kenwood\", \"Lake View\", \"Lincoln Park\", \"Lincoln Square\", \"Logan Square\", \"Loop\", \"Lower West Side\",\n",
    "        \"McKinley Park\", \"Montclare\", \"Morgan Park\", \"Mount Greenwood\", \"Near North Side\", \"Near South Side\", \"Near West Side\",\n",
    "        \"New City\", \"North Center\", \"North Lawndale\", \"North Park\", \"Norwood Park\", \"O'Hare\", \"Oakland\", \"Portage Park\",\n",
    "        \"Pullman\", \"Riverdale\", \"Rogers Park\", \"Roseland\", \"South Chicago\", \"South Deering\", \"South Lawndale\",\n",
    "        \"South Shore\", \"Streeterville\", \"Uptown\", \"Washington Heights\", \"Washington Park\", \"West Elsdon\",\n",
    "        \"West Englewood\",\"West Garfield Park\", \"West Lawn\", \"West Pullman\", \"West Ridge\", \"West Town\", \"Woodlawn\"\n",
    "    ]\n",
    "    address = address.replace(\"(\", \"\")\n",
    "    address=address.replace(\")\",\"\")\n",
    "    #print(address, 'togther')\n",
    "\n",
    "    location=geocoder.osm(address,method='reverse')\n",
    "    location=str(location)\n",
    "    #print(location,\"copy from here \")\n",
    "    location=location.split(\",\")\n",
    "    need=None\n",
    "    if location is not None:\n",
    "        for i in range(len(location)):\n",
    "            print(location[i].strip(),'the i is here')\n",
    "            if location[i]==\" Chicago\":\n",
    "                need=location[i-1]\n",
    "                #print(need,\"waht is need\")\n",
    "                break\n",
    "    if need is not None :\n",
    "        print(need, \"what we found\")\n",
    "    else :\n",
    "        print(\"no matches found\")\n",
    "    return need\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/exel_4000kdata_pls_work.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "for index, row in df.iterrows():\n",
    "    address = row['Location']\n",
    "    df.at[index, 'NEW LOCATION'] = get_address(address)\n",
    "#df['Location']=df['Location'].apply(translate_add)\n",
    "\n",
    "df.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/exel_4000kdata_translated.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### orgenizing the places to 5 major districts ###################\n",
    "#####then predictin other places only by long and lat cos it would take a lot of time to do it other way ####\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/exel_4000kdata_translated.csv'\n",
    "\n",
    "try:\n",
    "     the_file = pd.read_csv(path_to_file)\n",
    "     print(\"File opened successfully.\")\n",
    "except Exception as e:\n",
    "     print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "the_file = the_file.drop(the_file[the_file['Location'] == 'UNknown'].index)\n",
    "\n",
    "the_file[['Latitude', 'Longitude']] = the_file['Location'].str.split(',', expand=True)\n",
    "the_file['Latitude'] = the_file['Latitude'].astype(str)\n",
    "the_file['Longitude'] = the_file['Longitude'].astype(str)\n",
    "the_file['Latitude'] = the_file['Latitude'].str.strip()\n",
    "the_file['Longitude'] = the_file['Longitude'].str.strip()\n",
    "the_file['Latitude'] = the_file['Latitude'].str.replace('(', '')\n",
    "the_file['Longitude'] = the_file['Longitude'].str.replace(')', '')\n",
    "the_file['Latitude'] = pd.to_numeric(the_file['Latitude'])\n",
    "the_file['Longitude'] = pd.to_numeric(the_file['Longitude'])\n",
    "uniq_locations=the_file['NEW LOCATION'].unique()\n",
    "print(uniq_locations)\n",
    "mapping_for_location={\n",
    "    \" Loop\":\"Upper Town\",\n",
    "    \" Near\":\"Upper Town\",\n",
    "    \" North\":\"Upper Town\",\n",
    "    \" Side\":\"Upper Town\",\n",
    "    \" River\":\"Upper Town\",\n",
    "    \" North\":\"Upper Town\",\n",
    "    \" Streeterville\":\"Upper Town\",\n",
    "    \" Gold\":\"Upper Town\",\n",
    "    \" Coast\":\"Upper Town\",\n",
    "    \" Old\":\"Upper Town\",\n",
    "    \" Town\":\"Upper Town\",\n",
    "    \" Lincoln\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" Lake\":\"Upper Town\",\n",
    "    \" View\":\"Upper Town\",\n",
    "    \" Uptown\":\"Upper Town\",\n",
    "    \" North\":\"Upper Town\",\n",
    "    \" Center\":\"Upper Town\",\n",
    "    \" Lincoln\":\"Upper Town\",\n",
    "    \" Square\":\"Upper Town\",\n",
    "    \" Edgewater\":\"Upper Town\",\n",
    "    \" Rogers\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" West\":\"Upper Town\",\n",
    "    \" Ridge\":\"Upper Town\",\n",
    "    \" North\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" Albany\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" Jefferson\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" Norwood\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" Edison\":\"Upper Town\",\n",
    "    \" Park\":\"Upper Town\",\n",
    "    \" South Shore\":\"Lower Town\",\n",
    "    \" Hyde Park\":\"Lower Town\",\n",
    "    \" Woodlawn\":\"Lower Town\",\n",
    "    \" Kenwood\":\"Lower Town\",\n",
    "    \" Bronzeville\":\"Lower Town\",\n",
    "    \" Washington Park\":\"Lower Town\",\n",
    "    \" Fuller Park\":\"Lower Town\",\n",
    "    \" Grand Boulevard\":\"Lower Town\",\n",
    "    \" Englewood\":\"Lower Town\",\n",
    "    \" West Englewood\":\"Lower Town\",\n",
    "    \" Gage Park\":\"Lower Town\",\n",
    "    \" Chicago Lawn\":\"Lower Town\",\n",
    "    \" West Lawn\":\"Lower Town\",\n",
    "    \" Ashburn\":\"Lower Town\",\n",
    "    \" Marquette Park\":\"Lower Town\",\n",
    "    \" Garfield Ridge\":\"Lower Town\",\n",
    "    \" Clearing\":\"Lower Town\",\n",
    "    \" West Elsdon\":\"Lower Town\",\n",
    "    \" Archer Heights\":\"Lower Town\",\n",
    "    \" Brighton Park\":\"Lower Town\",\n",
    "    \" Austin\":\"West Side\",\n",
    "    \" Humboldt Park\":\"West Side\",\n",
    "    \" Garfield Park\":\"West Side\",\n",
    "    \" West Garfield Park\":\"West Side\",\n",
    "    \" North Lawndale\":\"West Side\",\n",
    "    \" East Garfield Park\":\"West Side\",\n",
    "    \" Belmont Cragin\":\"West Side\",\n",
    "    \" Hermosa\":\"West Side\",\n",
    "    \" Logan Square\":\"West Side\",\n",
    "    \" South Deering\":\"East Side\",\n",
    "    \" East Side\":\"East Side\",\n",
    "    \" South Chicago\":\"East Side\",\n",
    "    \" Hegewisch\":\"East Side\",\n",
    "    \" Riverdale\":\"East Side\",\n",
    "    \" Pullman\":\"East Side\",\n",
    "    \" Burnside\":\"East Side\",\n",
    "    \" Calumet Heights\":\"East Side\",\n",
    "    \" Roseland\":\"East Side\",\n",
    "    \" Greater Grand Crossing\":\"Middle City\",\n",
    "    \" Chatham\":\"Middle City\",\n",
    "    \" Avalon Park\":\"Middle City\",\n",
    "    \" South Shore Historical Bungalow District\":\"Middle City\",\n",
    "    \" South Commons\":\"Middle City\",\n",
    "    \" Bridgeport\":\"Middle City\",\n",
    "    \" McKinley Park\":\"Middle City\",\n",
    "    \" Washington Heights\":\"Middle City\",\n",
    "    \" Morgan Park\":\"Middle City\",\n",
    "    \" Beverly\":\"Middle City\",\n",
    "    \" Mount Greenwood\":\"Middle City\",\n",
    "    \" Washington Park\":\"Middle City\",\n",
    "    \" Oakland\":\"Middle City\",\n",
    "    \" Douglas\":\"Middle City\",\n",
    "    \" Kenwood\":\"Middle City\",\n",
    "    \" Grand Boulevard\":\"Middle City\",\n",
    "    \" Fuller Park\":\"Middle City\",\n",
    "    \" Woodlawn\":\"Middle City\",\n",
    "    \" Bronzeville\":\"Middle City\",\n",
    "    \" Chicago Lawn\":\"Middle City\",\n",
    "    \" Avondale\": \"Upper Town\",\n",
    "    \" Auburn Gresham\": \"Lower Town\",\n",
    "    \" South Lawndale\": \"Lower Town\",\n",
    "    \" Lower Town\": \"Lower Town\",\n",
    "    \" West Side\": \"West Side\",\n",
    "    \" Middle City\": \"Middle City\",\n",
    "    \" Jefferson Park\": \"Upper Town\",\n",
    "    \" East Side\": \"East Side\",\n",
    "    \" West Town\": \"West Town\",\n",
    "    \" Portage Park\": \"Upper Town\",\n",
    "    \" Forest Glen\": \"Upper Town\",\n",
    "    \" Midway Village\": \"Middle City\",\n",
    "    \" Albany Park\": \"Upper Town\",\n",
    "    \" West Ridge\": \"Upper Town\",\n",
    "    \" Near North Side\": \"Upper Town\",\n",
    "    \" Lake View\": \"Upper Town\",\n",
    "    \" Near West Side\": \"Upper Town\",\n",
    "    \" West Pullman\": \"Lower Town\",\n",
    "    \" Near South Side\": \"Upper Town\",\n",
    "    \" North Park\": \"Upper Town\",\n",
    "    \" River North\": \"Upper Town\",\n",
    "    \" Lincoln Park\": \"Upper Town\",\n",
    "    \" North Center\": \"Upper Town\",\n",
    "    \" Rogers Park\": \"Upper Town\",\n",
    "    \" Norwood Park\": \"Upper Town\",\n",
    "    \" Bryn Mawr East\": \"Upper Town\",\n",
    "    \" Altgeld Gardens\": \"Lower Town\",\n",
    "    \" Lower West Side\": \"Lower Town\",\n",
    "    \" Parkside\": \"Upper Town\",\n",
    "    \" London Towne Houses\": \"Upper Town\",\n",
    "    \" Beat 2523\": \"Upper Town\",\n",
    "    \" Andersonville\": \"Upper Town\",\n",
    "    \" Kinzie Industrial Corridor\": \"Upper Town\",\n",
    "    \" Armour Square\": \"Upper Town\",\n",
    "    \" Irving Park\": \"Upper Town\",\n",
    "    \" Beat 2512\": \"Upper Town\",\n",
    "    \" Lincoln Square\": \"Upper Town\",\n",
    "    \" Dunning\": \"Upper Town\",\n",
    "    \" Bryn Mawr West\": \"Upper Town\",\n",
    "    \" Michigan Ave\": \"Upper Town\",\n",
    "    \" Six Corners\": \"Upper Town\",\n",
    "    \" Montclare\": \"Upper Town\",\n",
    "    \" O'Keeffe\": \"Upper Town\",\n",
    "    \" The Yards Plaza\": \"Upper Town\",\n",
    "    \" O'Hare\": \"Upper Town\",\n",
    "    \" Beat 2521\": \"Upper Town\",\n",
    "    \" Beat 2522\": \"Upper Town\",\n",
    "    \" The Island\": \"Upper Town\",\n",
    "    \" Beat 2515\": \"Upper Town\",\n",
    "    \" The Grant\": \"Upper Town\",\n",
    "    \" Catherine Courts Apartments\": \"Upper Town\",\n",
    "    \" Edison Park\": \"Upper Town\",\n",
    "    \" Beat 2532\": \"Upper Town\",\n",
    "    \" Stockyards Industrial Corridor\": \"Upper Town\",\n",
    "    \" Beat 2514\": \"Upper Town\",\n",
    "    \" Jeffrey - Cyril Historic District\": \"Upper Town\",\n",
    "    \" Beat 2511\": \"Upper Town\",\n",
    "    \" Concordia Place Apartments\": \"Upper Town\",\n",
    "    \" Chinatown\": \"Upper Town\",\n",
    "    \" Dearborn Homes\": \"Upper Town\",\n",
    "    \" Jackson Park Highlands District\": \"Upper Town\",\n",
    "    \" Madison Terrace Apartments\": \"Upper Town\",\n",
    "    \" Martin Luther King Jr Plaza Apartments\": \"Upper Town\",\n",
    "    \" Gateway Centre Plaza\": \"Upper Town\",\n",
    "    \" Lake Meadows\": \"Upper Town\",\n",
    "    \" Beat 2535\": \"Upper Town\",\n",
    "    \" Essex\": \"Upper Town\",\n",
    "    \" Chatham Park Village Cooperative\": \"Upper Town\",\n",
    "    \" Peoples Gas\": \"Upper Town\",\n",
    "    \" CTA West Shops\": \"Upper Town\",\n",
    "    \" New City\":\"Middle City\",\n",
    "\n",
    "}\n",
    "the_file['simple location']=the_file['NEW LOCATION'].replace(mapping_for_location)\n",
    "print(\"new unique in simple location \",the_file['simple location'].unique())\n",
    "keep=['Middle City', 'West Side', 'Upper Town', 'Lower Town', 'East Side']\n",
    "mask = ~the_file['simple location'].isin(keep)\n",
    "the_file.drop(the_file[mask].index, inplace=True)\n",
    "print(\"new unique in simple location \",the_file['simple location'].unique())\n",
    "\n",
    "the_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/exel_4000kdata_translated_and_seperated.csv', index=False)\n",
    "\n",
    "x = the_file[['Latitude', 'Longitude']]\n",
    "y = the_file['simple location']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "     'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_depth': [None, 5, 10, 15, 20]\n",
    " }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, scoring='accuracy')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "accuracy = best_decision_tree.score(x_test, y_test)\n",
    "print(\"Test Set Accuracy: \", accuracy)\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/importent copy dont touch/full_data_upgraded_cleaned.csv'\n",
    "\n",
    "try:\n",
    "     big_file = pd.read_csv(path_to_file)\n",
    "     print(\"File opened successfully.\")\n",
    "except Exception as e:\n",
    "     print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "mini_big_file=big_file.head(65000)\n",
    "try:\n",
    "    mini_big_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big.csv', index=False)\n",
    "    print(\"we did save it so it hasse to bi some where there \")\n",
    "\n",
    "except:\n",
    "    print(\"Failed to save file\")\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big.csv'\n",
    "\n",
    "try:\n",
    "     big_file = pd.read_csv(path_to_file)\n",
    "     print(\"XXXXXXXXXXXFile opened successfully.XXXXX\")\n",
    "except Exception as e:\n",
    "     print(\"Error occurred while opening the file:\", e)\n",
    "try:\n",
    "\n",
    "    mini_big_file[['Latitude', 'Longitude']] = mini_big_file['Location'].str.split(',', expand=True)\n",
    "    mini_big_file['Latitude'] = mini_big_file['Latitude'].astype(str)\n",
    "    mini_big_file['Longitude'] = mini_big_file['Longitude'].astype(str)\n",
    "    mini_big_file['Latitude'] = mini_big_file['Latitude'].str.strip()\n",
    "    mini_big_file['Longitude'] = mini_big_file['Longitude'].str.strip()\n",
    "    mini_big_file['Latitude'] = mini_big_file['Latitude'].str.replace('(', '')\n",
    "    mini_big_file['Longitude'] = mini_big_file['Longitude'].str.replace(')', '')\n",
    "    mini_big_file['Latitude'] = pd.to_numeric(mini_big_file['Latitude'])\n",
    "    mini_big_file['Longitude'] = pd.to_numeric(mini_big_file['Longitude'])\n",
    "\n",
    "\n",
    "    print(\"we did all the bullshit it should be there\")\n",
    "except:\n",
    "    print(\"we failed some where in the bullshit \")\n",
    "try:\n",
    "        mini_big_file = mini_big_file.drop(mini_big_file[mini_big_file['Location'] == 'UNknown'].index)\n",
    "        print(\"it worked the unknown should be deleted\")\n",
    "except:\n",
    "        print(\"removing failed for unknown\")\n",
    "\n",
    "x_predict = mini_big_file[['Latitude', 'Longitude']]\n",
    "mini_big_file['simple location'] = best_decision_tree.predict(x_predict)\n",
    "\n",
    "try:\n",
    "\n",
    "    mini_big_file.to_csv('D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_saparated.csv', index=False)\n",
    "    print(\"we saved the file at the end \")\n",
    "except:\n",
    "    print(\"we failed to save this end file \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### first try machin learning ##########LogisticRegression###############decision tree############\n",
    "file_path = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/test_items_try_muchinlearing.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df['Primary Type'].dtype)\n",
    "print(df['Location Description'].dtype)\n",
    "print(df['Description'].dtype)\n",
    "print(df['Hour'].dtype)\n",
    "\n",
    "x = df[['Primary Type', 'Location Description', 'Description']]\n",
    "y = df['Hour']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy Score:\", acc_score)\n",
    "\n",
    "x = df[['Primary Type', 'Location Description', 'Description']]\n",
    "y = (df['Hour'] >= 12).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy Score:\", acc_score)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)\n",
    "y_pred = regression.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Linear Regression Mean Squared Error:\", mse)\n",
    "print(\"Linear Regression R-squared Score:\", r2)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"KNN Accuracy Score:\", accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.title('KNN: Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "best_score = 0\n",
    "best_depth = 0\n",
    "best_min_samples_split = 0\n",
    "\n",
    "for depth in range(1, 11):\n",
    "    for min_samples_split in range(2, 11):\n",
    "        tree = DecisionTreeClassifier(max_depth=depth, min_samples_split=min_samples_split)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_depth = depth\n",
    "            best_min_samples_split = min_samples_split\n",
    "\n",
    "print(\"Best Accuracy Score:\", best_score)\n",
    "print(\"Best Tree Depth:\", best_depth)\n",
    "print(\"Best Minimum Samples Split:\", best_min_samples_split)\n",
    "\n",
    "# Train and evaluate the decision tree with the best parameters\n",
    "tree_best = DecisionTreeClassifier(max_depth=best_depth, min_samples_split=best_min_samples_split)\n",
    "tree_best.fit(X_train, y_train)\n",
    "y_pred_best = tree_best.predict(X_test)\n",
    "\n",
    "acc_score_best = accuracy_score(y_test, y_pred_best)\n",
    "print(\"Decision Tree Accuracy Score:\", acc_score_best)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "####### second try machin learning #########\n",
    "\n",
    "path_to_file = 'D:/programs/CHROM EXECUTEBLES/files for project/fullstart/4k data/mini_big_try_this_for_learning_machin.csv'\n",
    "\n",
    "try:\n",
    "    big_file = pd.read_csv(path_to_file)\n",
    "    print(\"File opened successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while opening the file:\", e)\n",
    "\n",
    "x = big_file[['Primary Type', 'Description', 'Location Description', 'Hour', 'Latitude', 'Longitude', 'Domestic', 'Arrest']]\n",
    "y = big_file['simple location']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_depth': [1, 2, 3, 4, 5,]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, scoring='accuracy')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "accuracy = best_decision_tree.score(x_test, y_test)\n",
    "print(\"Decision Tree Test Set Accuracy: \", accuracy)\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=1000)\n",
    "logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "accuracy = logistic_regression.score(x_test, y_test)\n",
    "print(\"Logistic Regression Test Set Accuracy: \", accuracy)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Create an array to indicate correct predictions\n",
    "correct_predictions = predictions == y_test\n",
    "\n",
    "# Calculate accuracy for each label\n",
    "label_accuracy = {}\n",
    "unique_labels = np.unique(y_test)\n",
    "for label in unique_labels:\n",
    "    label_accuracy[label] = np.mean(correct_predictions[y_test == label])\n",
    "\n",
    "# Plot bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_accuracy.keys(), label_accuracy.values())\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Prediction Accuracy for Each Label\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
